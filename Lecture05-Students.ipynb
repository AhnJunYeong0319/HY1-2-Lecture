{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Method: Calculating the area under the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First integral \n",
    "\n",
    "**Task**: resolve the integral \n",
    "\n",
    "$\\int_1^5 x^2 dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by resolving this integral using the standard analytical method, assisted by the [SymPy symbolic mathematics library](https://sympy.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy.interactive import printing\n",
    "printing.init_printing(use_latex=\"mathjax\")\n",
    "\n",
    "x = sympy.Symbol(\"x\")\n",
    "sympy.integrate(x**2, (x, 1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression we wish to integrate is very simple here, so calculating its integral analytically is easy (even without resorting to Python’s symbolic mathematics package!). In many cases, however, analytical approaches to integration are not feasible:\n",
    "\n",
    "- the expression we wish to integrate is very complicated, possibly without a closed analytical integral\n",
    "- it is only known in “black box” form (for instance a software module): we can evaluate it at various points but don’t know the corresponding equation\n",
    "\n",
    "In such situations, stochastic simulation (“Monte Carlo”) methods allow us to generate an approximation of the integral, simply by evaluating the expression a large number of times at randomly selected points in the input space and counting the proportion that are less than the integrand at that point. The larger the number of simulations we run, the better the approximation (and note that computers are very very fast today, so on simple problems the simulation will run in a blink of an eye!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100_000\n",
    "accum = 0\n",
    "for i in range(N):\n",
    "......\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second integral\n",
    "\n",
    "**Task**: resolve the integral $\\int_3^2 x^2 + 4 x sin(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sympy.Symbol(\"x\")\n",
    "float(sympy.integrate(x**2 + 4 * x * sympy.sin(x), (x, 2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100_000\n",
    "accum = 0 \n",
    "for i in range(N):\n",
    "......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: now undertake the same comparison of analytical and stochastic simulation methods to evaluate the integral\n",
    "\n",
    "$$\\int_1^3 e^{x^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Hint: the result should be approximately 1464.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A 2D integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move to a 2D integral, $\\int\\int cos(x^4) + 3y^2 dx dy$ over $x ∈ [4,6]$ and $y ∈ [0, 1]$. \n",
    "\n",
    "Monte Carlo integration methods become increasingly attractive when compared to other numerical integration methods (such as quadrature) as the number of dimensions increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s examine the **analytical solution**, again thanks to sympy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sympy.Symbol(\"x\")\n",
    "y = sympy.Symbol(\"y\")\n",
    "float(sympy.integrate(sympy.cos(x**4) + 3 * y**2, (x, 4, 6), (y, 0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare with **Monte Carlo estimation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100_000\n",
    "accum = 0\n",
    "for i in range(N):\n",
    "......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Detection Theory with ROC curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import necessary modules\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "#import statistics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_roc_curve(signalMean, noiseMean, variance):\n",
    "    criterionList = np.linspace(-signalMean-3*np.sqrt(variance),signalMean+3*np.sqrt(variance),100)\n",
    "    ## probability of correct rejection (pCR)\n",
    "    ## probability of target hit (pHit)\n",
    "    ## estimated dprime\n",
    "    C = len(criterionList)\n",
    "    pCR  = np.zeros(C)\n",
    "    pHit = np.zeros(C)\n",
    "    dPrimeEst = np.zeros(C)\n",
    "    nTrials = 100\n",
    "    for criterionNum, criterion in enumerate(criterionList):\n",
    "        criterion = criterionList[criterionNum]\n",
    "        ## randomly generate stimulation 0 (noise) or 1 (signal)\n",
    "        stim = np.zeros(nTrials)\n",
    "        stim[np.random.rand(nTrials) >0.5] = 1\n",
    "        ## initialization of response and internal response\n",
    "        resp = np.zeros(nTrials)\n",
    "        internalResponse = np.zeros(nTrials)\n",
    "        for t in range(nTrials):\n",
    "            if stim[t] == 0: ## no signal, only noise\n",
    "                internalResponse[t] = _____________\n",
    "            else: ## signal\n",
    "                internalResponse[t] = _____________\n",
    "            ## decide the response\n",
    "            resp[t] = _____________\n",
    "\n",
    "        ## Actual (from simulation experiment) probability of correct Rejection\n",
    "        pCR[criterionNum] = _____________\n",
    "        pFA = _____________\n",
    "        \n",
    "        if pCR[criterionNum]==0.0 or pCR[criterionNum]==1.0:\n",
    "            zCR = np.nan\n",
    "        else:  ## Calculate z-score of Correct Rejection probability using inverted cumulative Gaussian\n",
    "            #zCR = statistics.NormalDist().inv_cdf(pCR[criterionNum])\n",
    "            zCR = stats.norm.ppf(pCR[criterionNum])\n",
    "\n",
    "        ## Actual (from simulation experiment) probability of Hits \n",
    "        pHit[criterionNum] = _____________\n",
    "        if pHit[criterionNum]==0.0 or pHit[criterionNum]==1.0:\n",
    "            zHit = np.nan\n",
    "        else: ## Calculate z-score of Correct Rejection probability using inverted cumulative Gaussian\n",
    "            #zHit = statistics.NormalDist().inv_cdf(pHit[criterionNum])\n",
    "            zHit = stats.norm.ppf(pHit[criterionNum])\n",
    "\n",
    "        ## Actual (from simulation experiment) d' from simulation           \n",
    "        dPrimeEst[criterionNum] =zCR + zHit\n",
    "    return pFA, pHit, criterionList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Theoretical probability of Hits and False Alarm for criterionList\n",
    "#pHitReal = np.array([1- statistics.NormalDist(signalMean, np.sqrt(variance)).cdf(c) for c in criterionList])\n",
    "#pFAReal = np.array([1- statistics.NormalDist(noiseMean, np.sqrt(variance)).cdf(c) for c in criterionList])\n",
    "noiseMean, variance = 0, 1\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for k, signalMean in enumerate(np.arange(3)):\n",
    "    for m, variance in enumerate(np.arange(0.5,1.6,0.5)):\n",
    "        pFA, pHit, criterionList = __________(__________, __________, _____________)\n",
    "        ## Theoretical probability of Hits and False Alarm for __________\n",
    "        pHitReal = np.array([1-stats.norm.cdf(c, signalMean, np.sqrt(variance)) for c in criterionList])\n",
    "        pFAReal = np.array([1-stats.norm.cdf(c, noiseMean, np.sqrt(variance)) for c in criterionList])\n",
    "    ## use np.trapz for calculation of area under ROC curve for the actual case\n",
    "        area_under_curve = ____________________\n",
    "        ax = fig.add_subplot(3,3,3*k+m+1)\n",
    "        ax.plot(pFA, pHit, 'b.')\n",
    "        ax.grid(True, linestyle='-.')\n",
    "        ax.tick_params(labelcolor='k', labelsize=15, width=2)\n",
    "\n",
    "    ## For theoretical ROC curve\n",
    "        plt.plot(pFAReal, pHitReal, 'k')\n",
    "        plt.xlabel('p (False Alarm)',fontsize=20)\n",
    "        plt.ylabel('p (Hit)',fontsize=20)\n",
    "        plt.text(0.45, 0.15, 'auc = {:04.3f}'.format(area_under_curve), size=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do the same thing with using modules from sklearn, metrics.roc_curve\n",
    "from sklearn import metrics\n",
    "....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture05-Pandas1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('homeless_data.pkl', 'rb') as f:\n",
    "    homelessness = \n",
    "with open('walmart_sales.pkl', 'rb') as f:\n",
    "    sales = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of the homelessness data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about homelessness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of homelessness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a description of homelessness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the values of homelessness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column index of homelessness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the row index of homelessness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sorting rows\n",
    "# Sort homelessness by individual\n",
    "homelessness_ind = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort homelessness by descending family members\n",
    "homelessness_fam = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort homelessness by region, then descending family members\n",
    "homelessness_reg_fam = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top few rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsetting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the individuals column\n",
    "individuals = homelessness[___]\n",
    "\n",
    "# Print the head of the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the state and family_members columns\n",
    "state_fam = \n",
    "\n",
    "# Print the head of the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = \n",
    "\n",
    "# Print the head of the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsetting rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = homelessness[homelessness[___]>___]\n",
    "\n",
    "# See the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where region is Mountain\n",
    "mountain_reg = \n",
    "\n",
    "# See the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where family_members is less than 1000 \n",
    "# and region is Pacific\n",
    "fam_lt_1k_pac = \n",
    "\n",
    "# See the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsetting rows by categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for rows in South Atlantic or Mid-Atlantic regions\n",
    "south_mid_atlantic = homelessness[homelessness['region'].___(___)]\n",
    "\n",
    "# See the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows in the Mojave Desert states\n",
    "mojave_homelessness = =\n",
    "\n",
    "# See the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding new columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add total col as sum of individuals and family_members\n",
    "homelessness['total'] = ___ + ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add p_individuals col as proportion of individuals\n",
    "homelessness['p_individuals'] = ___ / ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrapping up\n",
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = \n",
    "\n",
    "# See the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of the sales DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the info about the sales DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean of weekly_sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the median of weekly_sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the maximum of the date column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the minimum of the date column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Efficient summaries : usage of .agg() method\n",
    "def iqr(___):\n",
    "    return ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print IQR of the temperature_c column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping duplicates and Counting categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.___(subset=___)\n",
    "print(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of stores of each type\n",
    "store_counts = store_types[___].___()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the proportion of stores of each type\n",
    "store_props = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate store/department combinations\n",
    "store_depts = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each department number and sort\n",
    "dept_counts_sorted = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the proportion of departments of each number and sort\n",
    "dept_props_sorted = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the rows that are holiday weeks and drop duplicate dates\n",
    "holiday_dates = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print date col of holiday_dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What percent of sales occurred at each store type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc total weekly sales\n",
    "sales_all = sales[___].___()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for type A stores, calc total weekly sales\n",
    "sales_A = sales[sales[___][___].___()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for type B stores, calc total weekly sales\n",
    "sales_B = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for type C stores, calc total weekly sales\n",
    "sales_C = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proportion for each type\n",
    "sales_propn_by_type = [___, ___, ___] / ___\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better solution using .groupby()\n",
    "# Group by type; calc total weekly sales\n",
    "sales_by_type = sales.___(___)[___].___()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proportion for each type\n",
    "sales_propn_by_type = ___/___\n",
    "\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous step\n",
    "sales_by_type = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_propn_by_type_is_holiday = \n",
    "print(sales_propn_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy with the alias np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.___(___)[___].___([___, ___, ___, ___])\n",
    "\n",
    "# Print sales_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = \n",
    "\n",
    "# Print unemp_fuel_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = \n",
    "sales.___(values=, index=, aggfunc=, fill_value = , margins=)\n",
    "\n",
    "# Print sales_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = \n",
    "sales.pivot_table(___)\n",
    "\n",
    "# Print unemp_fuel_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_fuel_stats_is_holyday = \n",
    "sales.___\n",
    "\n",
    "# Print unemp_fuel_stats_is_holyday\n",
    "print(unemp_fuel_stats_is_holyday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.kaggle.com/sudalairajkumar/daily-temperature-of-major-cities\n",
    "\n",
    "temperatures= pd.read_csv('city_temperature.csv')\n",
    "\n",
    "# Look at temperatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index temperatures by city\n",
    "temperatures_ind = \n",
    "\n",
    "# Look at temperatures_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index, keeping its contents\n",
    "print(temperatures_ind.___)\n",
    "\n",
    "# Reset the index, dropping its contents\n",
    "print(temperatures_ind.___(drop=___))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subsetting with .loc[]\n",
    "# Make a list of cities, Seoul, Chicago, Osaka\n",
    "cities = [\"Seoul\", \"Chicago\", \"Osaka\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset temperatures using square brackets\n",
    "print(temperatures[temperatures[___].___(___)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.loc[cities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_ind.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Subsetting with .loc[]\n",
    "\n",
    "# Make a list of cities, Seoul, Chicago, Osaka\n",
    "cities = [\"Seoul\", \"Chicago\", \"Osaka\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset temperatures using square brackets\n",
    "print(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.___[___])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index temperatures by country & city\n",
    "temperatures_ind = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [('US', 'Los Angeles'),('Japan','Osaka')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for rows to keep\n",
    "print(temperatures_ind.___[___])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.___(level=___))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.___(level=, ascending = ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Slicing index values\n",
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset rows from Pakistan to Russia\n",
    "print(temperatures_srt.___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to subset rows f``rom Lahore to Moscow\n",
    "print(temperatures_srt.___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
    "print(temperatures_srt.___])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Slicing in both directions\n",
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.___])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset in both directions at once\n",
    "print(temperatures_srt.___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing time series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a colum to temperature named date in format of yy-mm-dd up to 10000 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set date as an index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_by_country_city_vs_year = temperatures.pivot_table(values='AvgTemperature', \n",
    "                                                        index=['Country', 'City'], \n",
    "                                                        columns = 'Year',fill_value=0)\n",
    "\n",
    "# See the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for Egypt to India\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for Egypt, Cairo to India, Delhi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset in both directions at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the worldwide mean temp by year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the year that had the highest mean temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean temp by city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the city that had the lowest mean temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Which avocado size is most popular?\n",
    "with open('avoplotto.pkl', 'rb') as f:\n",
    "    avocados = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first few rows of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of avocados sold of each size\n",
    "nb_sold_by_size = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot of the number of avocados sold by size\n",
    "nb_sold_by_size.___(___)\n",
    "\n",
    "# Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changes in sales over time\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "\n",
    "\n",
    "# Get the total number of avocados sold on each date\n",
    "nb_sold_by_date = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot of the number of avocados sold by date\n",
    "nb_sold_by_date.___(___)\n",
    "\n",
    "# Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of nb_sold vs avg_price with title\n",
    "\n",
    "# Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of conventional avg_price \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].___()\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[___][___].___(alpha=, bins=)\n",
    "\n",
    "# Add a legend\n",
    "plt.___([___, ___])\n",
    "\n",
    "# Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding missing values\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "avocados_2015 = avocados[avocados['year']==2015]\n",
    "print(avocados_2015.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each column for missing values\n",
    "print(avocados_2015.___().___())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of missing values by variable\n",
    "avocados_2015.___().___().___(kind=___)\n",
    "\n",
    "# Show plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creading dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries with new data\n",
    "avocados_list = [\n",
    "    {'date': \"2019-11-03\", 'small_sold': 10376832, 'large_sold':7835071},\n",
    "    {'date': \"2019-11-10\", 'small_sold': 10717154, 'large_sold':8561348},\n",
    "]\n",
    "\n",
    "# Convert list into DataFrame\n",
    "avocados_2019 = pd.DataFrame(___)\n",
    "\n",
    "# Print the new DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of lists with new data\n",
    "avocados_dict = {\n",
    "  \"date\": ['2019-11-17','2019-12-01'],\n",
    "  \"small_sold\": [10859987, 9291631],\n",
    "  \"large_sold\": [7674135, 6238096]\n",
    "}\n",
    "\n",
    "# Convert dictionary into DataFrame\n",
    "avocados_2019 = pd.DataFrame(___)\n",
    "\n",
    "# Print the new DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
